# -*- coding: utf-8 -*-
"""
# ---------------------------------------------------------------------------
# Nom : data_text_cleaning.py
# Creation : 02/07/2021
# Modification : 27/07/2021
# Auteur(s) : JÃ©ros VIGAN 
# Projet :
# Description : Nettoyage et formatage des textes en texte brute pour text_mining
# ---------------------------------------------------------------------------
"""
### ===============================================================
###  DÃ©claration du dossier de travail
### ===============================================================
import os
base = r'F:\Text_ML'
base = base.replace('\\', '/')
os.chdir(base)
os.getcwd()

### ===============================================================
### Importation des packages
### ===============================================================
from datetime import datetime
import pandas as pd
import string
import re
from unidecode import unidecode
from langdetect import detect
from textblob import TextBlob
from script_02_Preprocess import *
from script_03_Text_To_Speech import *
import spacy

### ===============================================================
###  Fonctions simples de nettoyage
### ===============================================================
abbreviations = {
    "$" : " dollar ",
    "â‚¬" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}

#fonctions de nettoyage des abbreviations
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

emojis = "ðŸ•ðŸµðŸ˜‘ðŸ˜¢ðŸ¶ï¸ðŸ˜œðŸ˜ŽðŸ‘ŠðŸ¤ªðŸ˜ðŸ˜ðŸ’–ðŸ’µðŸ‘ŽðŸ˜€ðŸ˜‚ðŸ”¥â­ðŸ¤¯ðŸ˜„ðŸ¤ªðŸ»ðŸ’¥ðŸ˜‹ðŸ‘ðŸ˜±ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ•ðŸ™€ðŸ˜ðŸ˜•ðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜ðŸ’©ðŸ’¯â›½ðŸš„ðŸ˜–ðŸ¼ðŸš²ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡ðŸ‘ŒðŸ™„ðŸ˜ ðŸ˜‰ðŸ˜¤â›ºðŸ™‚ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜žðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ªðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ðŸ’—ðŸ’šðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸â¤µðŸ†ðŸŽƒðŸ˜©ðŸ‘®ðŸ’™ðŸ¾ðŸ•ðŸ˜†ðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™ŒðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰ðŸš¬ðŸ¤“ðŸ˜µðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ðŸ˜²ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘ðŸ’¤ðŸ‡ðŸ¡â”â‰ðŸ‘ ã€‹ðŸ‡¹ðŸ‡¼ðŸŒ¸ðŸŒžðŸŽ²ðŸ˜›ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽðŸ—‘ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°ðŸ¤£ðŸðŸŽ…ðŸºðŸŽµðŸŒŽÍŸðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§ðŸš€ðŸ¤´ðŸ˜ðŸ’¨ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦ðŸ€ðŸ˜«ðŸ¤¤ðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ˜·ðŸ‡¨ðŸ‡¦ðŸŒðŸ“ºðŸ‹ðŸ’˜ðŸ’“ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ðŸ‘ºðŸ·ðŸš¶ðŸ¤˜Í¦ðŸ’¸ðŸ‘‚ðŸ‘ƒðŸŽ«ðŸš¢ðŸš‚ðŸƒðŸ‘½ðŸ˜™ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸ðŸ„ðŸ€ðŸš‘ðŸ¤·ðŸ¤™ðŸ’ðŸˆï·»ðŸ¦„ðŸš—ðŸ³ðŸ‘‡â›·ðŸ‘‹ðŸ¦ŠðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽðŸðŸ¦ðŸ™‹ðŸ˜¶ðŸ”«ðŸ‘ðŸ’²ðŸ—¯ðŸ‘‘ðŸš¿ðŸ’¡ðŸ˜¦ðŸðŸ‡°ðŸ‡µðŸ‘¾ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ðŸ­ðŸ‘£ðŸ‰ðŸ’­ðŸŽ¥ðŸ´ðŸ‘¨ðŸ¤³ðŸ¦ðŸ©ðŸ˜—ðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²ðŸ’ðŸ‘â°ðŸ’ŠðŸŒ¤ðŸŠðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ðŸ’¢ðŸ’’ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸðŸ¦ðŸ¡ðŸ’³á¼±ðŸ™‡ðŸ¥œðŸ”¼"

#fonctions de nettoyage des emojis
def remove_emojis(text):
    for emoji in emojis:
        text = text.replace(emoji, '')
    return text

def remove_emoji(text):
    emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'EMOJI', text)

regular_punct = list(string.punctuation)
extra_punct = [
    ',', '.', '"', ':', ')', '(', '!', '?', '|', ';', "'", '$', '&',
    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\', 'â€¢',  '~', '@', 'Â£',
    'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',
    'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦', 'â€œ', 'â˜…', 'â€',
    'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾',
    'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼',
    'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²',
    'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž', 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»',
    'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜',
    'Â¹', 'â‰¤', 'â€¡', 'âˆš', 'Â«', 'Â»', 'Â´', 'Âº', 'Â¾', 'Â¡', 'Â§', 'Â£', 'â‚¤']

all_punct = list(set(regular_punct + extra_punct))

#fonctions de nettoyage des ponctuations
def spacing_punctuation(text):
    for punc in all_punct:
        if punc in text:
            text = text.replace(punc, f' {punc} ')
    return text

def remove_repeat_punct(text):
    rep = re.compile(r'([!?.]){2,}')
    return rep.sub(r'\1 REPEAT', text)

def remove_all_punct(text):
    table = str.maketrans('','',string.punctuation)
    return text.translate(table)

def remove_punct(text):
    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + "'`" 
    for p in punctuations:
        text = text.replace(p, f' {p} ')
        text = text.replace('...', ' ... ')
        if '...' not in text:
            text = text.replace('..', ' ... ')   
    return text

#fonction de nettoyage des urls   
def remove_URL(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'URL',text)

#fonction de nettoyage des balises html
def remove_HTML(text):
    html=re.compile(r'<.*?>')
    return html.sub(r'',text)

#fonction de nettoyage des codes ASCII
def remove_not_ASCII(text):
    text = ''.join([word for word in text if word in string.printable])
    return text

#fonction de nettoyage des caractÃ¨res de mentions
def remove_mention(text):
    at=re.compile(r'@\S+')
    return at.sub(r'USER',text)

#fonction de nettoyage des numeros de telephone
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'NUMBER', text)

#fonction de nettoyage des transcriptions
def transcription_sad(text):
    eyes = "[8:=;]"
    nose = "['`\-]"
    smiley = re.compile(r'[8:=;][\'\-]?[(\\/]')
    return smiley.sub(r'SADFACE', text)

#fonction de nettoyage des des transcriptions de smile
def transcription_smile(text):
    eyes = "[8:=;]"
    nose = "['`\-]"
    #smiley = re.compile(r'[8:=;][\'\-]?[)dDp]')
    smiley = re.compile(r'#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}/i')
    return smiley.sub(r'SMILE', text)

#fonction de nettoyage des transcriptions de coeur
def transcription_heart(text):
    heart = re.compile(r'<3')
    return heart.sub(r'HEART', text)

#fonction de nettoyage des mots allongÃ©es
def remove_elongated_words(text):
    rep = re.compile(r'\b(\S*?)([a-z])\2{2,}\b')
    return rep.sub(r'\1\2 ELONG', text)

#fonction de nettoyage des caractÃ¨res spÃ©ciales
def clean_special_patterns(text):
    email_regex = re.compile(r"[\w.-]+@[\w.-]+")
    url_regex = re.compile(r"(http|www)[^\s]+")
    date_regex = re.compile(r"[\d]{2,4}[ -/:]*[\d]{2,4}([ -/:]*[\d]{2,4})?")
    text = url_regex.sub("", text)
    text = email_regex.sub("", text)
    text = date_regex.sub("", text)
    return text.strip

#fonction de correction sur anglais
def correct_word(text):
    if detect(text)=='en':
        return TextBlob(text).correct()
    else:
        return text

#fonction pour retirer les accents
def remove_accents(text):
    return unidecode(text)

#fonction globale de nettoyage
def clean_text(text):
    # Remove non text
    text = remove_URL(text)
    text = remove_HTML(text)
    #text = remove_not_ASCII(text)
    #text = clean_special_patterns(text)
    
    #Lower text, replace abbreviations
    text = str(text).lower()
    text = remove_accents(text)
    text = replace_abbrev(text)  
    text = remove_mention(text)
    # text = remove_number(text)
    
    #Remove emojis / smileys
    text = remove_emoji(text)
    text = transcription_sad(text)
    text = transcription_smile(text)
    text = transcription_heart(text)
    
    #Remove repeated puntuations / words
    text = remove_elongated_words(text)
    text = remove_repeat_punct(text)
    text = remove_all_punct(text)
    text = remove_punct(text)
    return text

# DÃ©but du programme
debut = datetime.now()

### ===============================================================
### Importation 
### ===============================================================
df=pd.read_csv("train_en.csv")
df_fr=pd.read_csv("train_fr.csv",sep="\t")

### ===============================================================
### Application 
### ===============================================================
df['texte_clean'] = df['text'].apply(clean_text)
df_fr['texte_clean'] = df_fr['texte'].apply(clean_text)

### ===============================================================
### Enregistrer en txt (vecteur des mots)
### ===============================================================
# df.to_csv('nettoyage.txt',encoding="utf-8",index=False)

def save_txt(file,df):
    myText = open(file, 'w',encoding="utf-8")
    for ligne in df:
        # print(ligne)
        # print("\n"+ str(detect(ligne)))
        myText.write(ligne+"\n")
    myText.close()

save_txt('nettoyage_en.txt',df['texte_clean'])
save_txt('nettoyage_fr.txt',df_fr['texte_clean'])

### ===============================================================
### Fonctions 
### ===============================================================

def choice_langue(text):
    if str(detect(text[2]))=='en':
        precoss_anglais=Preprocess_list_anglais(text)
        print("\n Traitement anglais")
        return precoss_anglais
    else:
        precoss_french=Preprocess_list_french(text,dicto=r'./dictionnaire.txt')
        print("\n Traitement franÃ§ais")
        return precoss_french

def choice_langue_audio(text):
    text=[t for t in text if t]
    text= ".\n".join(text)
    if str(detect(text))=='en':
        text_to_speech(text,language ='en', name="text_en.mp3")
        print("\n Traitement anglais")
    else:
        text_to_speech(text,language ='fr', name="text_fr.mp3")
        print("\n Traitement franÃ§ais")

precoss_anglais=choice_langue(df['texte_clean'])  
precoss_french=choice_langue(df_fr['texte_clean'])        

save_txt('nettoyage_en_1.txt',precoss_anglais)
save_txt('nettoyage_fr_1.txt',precoss_french)

# choice_langue_audio(precoss_anglais)
# choice_langue_audio(precoss_french)

# Fin du programme
print(" ")
print(f'La durÃ©e de traitement est {str(datetime.now()-debut)} s')